{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f4dfa0d7-b1e8-44f2-bd10-c62a9d67e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4487eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, features, labels, device=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx].to(self.device)\n",
    "        label = self.labels[idx].to(self.device)\n",
    "        return feature, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "532ad725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_features, output_features, dropout_rate=0.5):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(input_features, 256)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.encoder_layer_1 = nn.Linear(256, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.bottleneck_layer = nn.Linear(128, 64)\n",
    "\n",
    "        self.decoder_layer_1 = nn.Linear(64, 128)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.decoder_layer_2 = nn.Linear(128, 256)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(256, output_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.batch_norm1(self.input_layer(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.batch_norm2(self.encoder_layer_1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.bottleneck_layer(x))  \n",
    "\n",
    "        x = torch.relu(self.batch_norm3(self.decoder_layer_1(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = torch.relu(self.batch_norm4(self.decoder_layer_2(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = torch.relu(self.batch_norm1(self.input_layer(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.batch_norm2(self.encoder_layer_1(x)))\n",
    "        x = self.dropout2(x)\n",
    "        return self.bottleneck_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8c48a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_features, dropout_rate=0.5):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_features, 128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu((self.fc1(x))) \n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = torch.relu((self.fc2(x)))  \n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.relu((self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x) \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "097160e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "e33c0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(algorithm, train_loss, test_loss, accuracy, auc, file_name,description,disease,valid = False):\n",
    "    result = {\n",
    "        'Algorithm': algorithm,\n",
    "        'Disease': disease,\n",
    "        'Valid': valid,\n",
    "        'Train Loss': train_loss,\n",
    "        'Test Loss': test_loss,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'AUC': auc,\n",
    "        'Timestamp': datetime.now(),\n",
    "        'description':description\n",
    "    }\n",
    "    file_path = 'results\\\\' + file_name \n",
    "    if not os.path.exists(file_path):\n",
    "        results_df = pd.DataFrame([result])\n",
    "        results_df.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        results_df = pd.DataFrame([result])\n",
    "        results_df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, loss_fn, optimizer, disease, algorithm, description, device, epochs, patience, min_delta):\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    best_train_loss = np.inf\n",
    "    best_test_loss = np.inf\n",
    "    best_accuracy = 0\n",
    "    best_auc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_outputs.extend(outputs.cpu().numpy())\n",
    "                predicted = (outputs >= 0.5).float()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        test_loss /= len(test_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        auc = roc_auc_score(np.array(all_labels), np.array(all_outputs))\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f} \"\n",
    "              f\"Test Loss: {test_loss:.4f} \"\n",
    "              f\"Test Accuracy: {accuracy:.2f}% \"\n",
    "              f\"AUC: {auc:.4f}\")\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            best_train_loss = train_loss\n",
    "            best_test_loss = test_loss\n",
    "            best_accuracy = accuracy\n",
    "            best_auc = auc\n",
    "\n",
    "    print(f\"BestEpoch [{best_epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {best_train_loss:.4f} \"\n",
    "              f\"Test Loss: {best_test_loss:.4f} \"\n",
    "              f\"Test Accuracy: {best_accuracy:.2f}% \"\n",
    "              f\"AUC: {best_auc:.4f}\")\n",
    "    save_results_to_csv(algorithm,\n",
    "                       train_loss=best_train_loss,\n",
    "                       test_loss=best_test_loss,\n",
    "                       accuracy=best_accuracy,\n",
    "                       auc=best_auc,\n",
    "                       file_name='result.csv',\n",
    "                       disease=disease, valid=True,\n",
    "                       description=description)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8f7ccf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log_transform(features):\n",
    "    return np.log(features + 1e-6) \n",
    "\n",
    "def apply_variance_thresholding(feature,variance_threshold):\n",
    "    feature_variances = np.var(feature, axis=0) \n",
    "    feature = feature[:, feature_variances > variance_threshold]\n",
    "    return feature\n",
    "\n",
    "def select_top_k_feature_from_random_forest(n_estimators,feature,ratio,df):\n",
    "        rf = RandomForestClassifier(n_estimators,random_state=42)\n",
    "        rf.fit(feature, df['disease'].apply(lambda x: 0 if x == 'healthy' else 1))\n",
    "        feature_importances = rf.feature_importances_\n",
    "        total_features = feature.shape[1]\n",
    "        k = math.floor((total_features*ratio))  \n",
    "        top_k_features = np.argsort(feature_importances)[-k:]\n",
    "        feature = feature[:, top_k_features]\n",
    "        return feature\n",
    "    \n",
    "def select_feature_from_autoencoder(model, feature, epochs=500, device=device):\n",
    "    earlyStopper = EarlyStopper(patience=30,min_delta=0)\n",
    "    model.train()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    feature_tensor = torch.Tensor(feature).to(device=device)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        acc_loss = 0\n",
    "        for batch in DataLoader(feature_tensor, batch_size=128, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "            batch = batch.to(device=device)\n",
    "            output = model(batch)\n",
    "            loss = loss_fn(output, batch)\n",
    "            acc_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_loss = acc_loss / len(feature_tensor) \n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.5f}\")\n",
    "\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0  \n",
    "            patience_counter += 1\n",
    "\n",
    "        if earlyStopper.early_stop(loss):\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    encoded_features = model.encode(feature_tensor)\n",
    "    return encoded_features.detach().cpu().numpy()\n",
    "\n",
    "def apply_normalization(features):\n",
    "    feature_min = np.min(features, axis=0)\n",
    "    feature_max = np.max(features, axis=0)\n",
    "    feature = (features - feature_min) / (feature_max - feature_min + 1e-6)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "080016de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_LN(features):\n",
    "    features = apply_log_transform(features=features)\n",
    "    features = apply_normalization(features=features)\n",
    "    return features\n",
    "    \n",
    "def apply_LVNRF(feature,df):\n",
    "    feature = apply_log_transform(features=feature)\n",
    "    feature = apply_variance_thresholding(feature=feature,variance_threshold=0.1)\n",
    "    feature = apply_normalization(features=feature)\n",
    "    feature = select_top_k_feature_from_random_forest(n_estimators=500,feature=feature,ratio=(3/4),df=df)\n",
    "    return feature\n",
    "def apply_LVAE(feature):\n",
    "    feature = apply_log_transform(features=feature)\n",
    "    feature = apply_variance_thresholding(feature=feature,variance_threshold=0.1)\n",
    "    input_features = feature.shape[1]\n",
    "    autoencoder = AutoEncoder(input_features=input_features, output_features=input_features).to(device=device)\n",
    "    feature = select_feature_from_autoencoder(model=autoencoder, feature=feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Define log----------------------\n",
    "description = [\n",
    "    \"\", \n",
    "    '10fold_log_transform+normalization', \n",
    "    'varinace=0.2,500 trees,k = 3/4',\n",
    "    'varinace=0.2,500 trees,k = 3/4'\n",
    "]\n",
    "algorithms = [\n",
    "    '10fold_raw', \n",
    "    '10fold_log_transform+normalization', \n",
    "    '10fold_log_transform+variance_threshold+normalization+random_forest',\n",
    "    '10fold_log_transform+variance_threshold+auto_encoder+normalization'\n",
    "]\n",
    "#Define number of folds\n",
    "k = 5\n",
    "\n",
    "#-----------Define disease file map---------------\n",
    "disease_file_map = {\n",
    "    'Cirrhosis': 'cirrhosis_10fold.csv',\n",
    "    'CRC': 'CRC_10fold.csv',\n",
    "    'CRC_AUS': 'CRC_AUS_LOSO.csv',\n",
    "    'CRC_CHI': 'CRC_CHI_LOSO.csv',\n",
    "    'CRC_FRA': 'CRC_FRA_LOSO.csv',\n",
    "    'CRC_GER': 'CRC_GER_LOSO.csv',\n",
    "    'CRC_IND': 'CRC_IND_additional.csv',\n",
    "    'CRC_USA': 'CRC_USA_LOSO.csv',\n",
    "    'IBD': 'IBD_10fold.csv',\n",
    "    'IBD_DK': 'IBD_DK_LOSO.csv',\n",
    "    'IBD_UK': 'IBD_UK_LOSO.csv',\n",
    "    'Obt': 'Obt_10fold.csv',\n",
    "    'T2D': 'T2D_10fold.csv'\n",
    "}\n",
    "for num_algorithms in range(len(algorithms)):\n",
    "    \n",
    "    for disease, file_name in disease_file_map.items():\n",
    "        \n",
    "        if(disease != 'IBD' or algorithms[num_algorithms] != '10fold_log_transform+normalization'):\n",
    "            continue\n",
    "         \n",
    "        #---------- read data from csv-----------------\n",
    "        raw_df = pd.read_csv('data\\\\' + disease_file_map[disease])\n",
    "        feature = raw_df.iloc[:, 4:].values \n",
    "        \n",
    "        if(algorithms == 1):\n",
    "            feature = apply_LN(features=feature)\n",
    "            \n",
    "        if(algorithms == 2):\n",
    "            feature = apply_LVNRF(feature=feature,df = raw_df)\n",
    "            \n",
    "        if(algorithms == 3):\n",
    "            feature = apply_LVAE(feature=feature)\n",
    "        \n",
    "        #\n",
    "        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        fold = 0\n",
    "        \n",
    "        for train_index, test_index in kf.split(feature,raw_df['disease']):\n",
    "            \n",
    "                \n",
    "            \n",
    "            print('-------------algorithm: ' + str(num_algorithms) + '| disease: ' + disease + ' | ' + 'Fold: ' + str(fold+1))\n",
    "\n",
    "        #---------- read data from csv-----------------\n",
    "            \n",
    "            fold_df = raw_df.loc[train_index,:]\n",
    "            fold_df['disease'] = fold_df['disease'].apply(lambda x: 0 if x == 'healthy' else 1)\n",
    "            feature = fold_df.iloc[:, 4:].values \n",
    "     \n",
    "            \n",
    "        # #---------define k-folds cross validation\n",
    "\n",
    "\n",
    "            #---------- convert label and feature into tensor-----------------\n",
    "            label = torch.tensor(fold_df.loc[:, 'disease'].values).unsqueeze(1).float().to(device=device) \n",
    "            feature = torch.Tensor(feature).to(device=device)\n",
    "\n",
    "\n",
    "            #----------split tensor into training set and test set-----------------\n",
    "            feature_train, feature_test, label_train, label_test = train_test_split(feature, label, test_size=0.1)\n",
    "\n",
    "            #----------define data loader------------------\n",
    "            train_loader =  DataLoader(TensorDataset(feature_train,label_train), batch_size=64, shuffle=True)\n",
    "            test_loader = DataLoader(TensorDataset(feature_test,label_test), batch_size=64, shuffle=True)\n",
    "\n",
    "            #----------define classifier model-----------------\n",
    "            classifier = BinaryClassifier(input_features=feature_train.shape[1]).to(device=device)\n",
    "\n",
    "\n",
    "            #---------define loss function and optimizer----------\n",
    "            loss_fn = nn.BCELoss()\n",
    "            optimizer = torch.optim.Adam(\n",
    "                classifier.parameters(),\n",
    "                lr=0.0001,\n",
    "                betas=(0.9, 0.999),\n",
    "                eps=1e-8,\n",
    "                weight_decay=1e-5\n",
    "            )\n",
    "\n",
    "            #---------train the classifier model-------------\n",
    "            train_classifier(\n",
    "                model=classifier,\n",
    "                train_loader=train_loader,\n",
    "                test_loader=test_loader,\n",
    "                epochs=1000,\n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                disease=disease,\n",
    "                algorithm=algorithms[num_algorithms],\n",
    "                description = description[num_algorithms],\n",
    "                patience=40,\n",
    "                min_delta=0.001,\n",
    "                device=device\n",
    "        )\n",
    "            fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('results\\\\result.csv')\n",
    "\n",
    "# Colors for each algorithm; use dark green for the AE bar, blue for AE error line\n",
    "colors = ['black', 'red', '#006400', 'darkblue']\n",
    "error_colors = ['red', 'black', 'blue', 'orange']  # Error color for LVN-RF2\n",
    "\n",
    "# Get unique diseases\n",
    "diseases = data['Disease'].unique()\n",
    "\n",
    "# Split diseases into three groups\n",
    "disease_groups = [\n",
    "    diseases[:4],   # First 4 diseases\n",
    "    diseases[4:8],  # Next 4 diseases\n",
    "    diseases[8:]    # Remaining diseases (5)\n",
    "]\n",
    "\n",
    "def create_single_group_plot(disease_group_data, diseases_in_group, group_idx):\n",
    "    num_diseases = len(diseases_in_group)\n",
    "    \n",
    "    # Create the plot with sufficient space for the groups\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Set the background to white\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    positions = np.arange(num_diseases)  # The x locations for the diseases\n",
    "    width = 0.2  # The width of the bars (adjusted for four bars)\n",
    "\n",
    "    # Lists to store means and std deviations for each algorithm\n",
    "    algorithm_means = [[] for _ in algorithms]\n",
    "    algorithm_stds = [[] for _ in algorithms]\n",
    "\n",
    "    for disease in diseases_in_group:\n",
    "        disease_data = disease_group_data[disease_group_data['Disease'] == disease]\n",
    "        \n",
    "        # Calculate statistics for each algorithm\n",
    "        for algo_idx, algo in enumerate(algorithms):\n",
    "            algo_data = disease_data[disease_data['Algorithm'] == algo]\n",
    "            auc_values = algo_data['AUC'].astype(float)\n",
    "            mean_auc = auc_values.mean()\n",
    "            std_auc = auc_values.std()\n",
    "            \n",
    "            algorithm_means[algo_idx].append(mean_auc)\n",
    "            algorithm_stds[algo_idx].append(std_auc)\n",
    "    \n",
    "    # Create the bars for each algorithm\n",
    "    bars = []\n",
    "    for i, (means, stds, color, err_color) in enumerate(zip(algorithm_means, algorithm_stds, colors, error_colors)):\n",
    "        bars.append(ax.bar(\n",
    "            positions + (i - 1.5) * width, means, width, color=color, yerr=stds,\n",
    "            error_kw=dict(elinewidth=2, ecolor=err_color)\n",
    "        ))\n",
    "\n",
    "    # Set the axis labels and title\n",
    "    ax.set_xlabel('Diseases', color='black')\n",
    "    ax.set_ylabel('AUC', color='black')\n",
    "    ax.set_title('AUC Performance Comparison Across Diseases and Algorithms', color='black')\n",
    "    ax.set_xticks(positions)\n",
    "    ax.set_xticklabels(diseases_in_group, rotation=45, color='black')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Ensure axis lines and ticks are visible\n",
    "    ax.spines['top'].set_visible(True)\n",
    "    ax.spines['right'].set_visible(True)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    \n",
    "    # Make sure all text is black\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_color('black')\n",
    "    \n",
    "    # Add the text labels inside the bars (centered horizontally and vertically) with rotation\n",
    "    algo_labels = ['R', 'LN', 'LVN-RF', 'LVN-AE']\n",
    "    for i, (bar_group, label) in enumerate(zip(bars, algo_labels)):\n",
    "        for rect in bar_group:\n",
    "            height = rect.get_height()\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2, height / 2, label,\n",
    "                ha='center', va='center', color='white', fontweight='bold', rotation=90  # Rotate label by 90 degrees\n",
    "            )\n",
    "            # Add the maximum value at the top of the bar\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2, height + 0.02, f'{height:.2f}',\n",
    "                ha='center', va='bottom', color='black', fontweight='bold', fontsize=8  # Adjust fontsize as needed\n",
    "            )\n",
    "    \n",
    "    # Save the figure\n",
    "    os.makedirs('graphs', exist_ok=True)\n",
    "    fig.savefig(f'{\"graphs\"}/batch_norm_10_fold_auc_comparison_group_{group_idx}.png', bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.close(fig)\n",
    "\n",
    "# Create plots for each disease group\n",
    "for group_idx, disease_group in enumerate(disease_groups):\n",
    "    group_data = data[data['Disease'].isin(disease_group)]\n",
    "    create_single_group_plot(group_data, disease_group, group_idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
